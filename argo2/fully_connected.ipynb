{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[:int(n * 0.8)]\n",
    "        \n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[:int(n * 0.8)]\n",
    "        \n",
    "    elif split == 'val':\n",
    "        f_in = ROOT_PATH + 'train' + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[int(n * 0.8):]\n",
    "        \n",
    "        f_out = ROOT_PATH + 'train' + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[int(n * 0.8):]\n",
    "    \n",
    "    else:\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07579624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056cee79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 20 17:26:10 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 384.81                 Driver Version: 460.67                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:B1:00.0 Off |                  N/A |\n",
      "|  0%   31C    P8     8W / 250W |      3MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "Internal error\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5b7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class Pred(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 120)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 100).float()\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.reshape(-1, 60, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa57f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pred, opt, train_dataset, train_loader, val_dataset, val_loader):\n",
    "    device = torch.device('cuda:0')\n",
    "    pred = pred.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(50):\n",
    "\n",
    "        total_loss = 0\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            inp, out = sample_batch\n",
    "            out = out.to(device)\n",
    "            inp = inp.to(device)\n",
    "            preds = pred(inp)\n",
    "            loss = ((preds - out) ** 2).sum()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        for i_batch, sample_batch in enumerate(val_loader):\n",
    "            inp, out = sample_batch\n",
    "            out = out.to(device)\n",
    "            inp = inp.to(device)\n",
    "            preds = pred(inp)\n",
    "            #print(preds)\n",
    "            loss = ((preds - out) ** 2).sum()\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        train_loss = np.log(total_loss / len(train_dataset))\n",
    "        val_loss = np.log(val_loss / len(val_dataset))\n",
    "\n",
    "        print('epoch {} train_loss: {} val_loss: {}'.format(epoch, train_loss, val_loss))\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(train_losses, color =\"red\", label = \"train_loss\")\n",
    "    plt.plot(val_losses, color =\"blue\", label = \"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564cd63b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city: austin\n",
      "epoch 0 train_loss: 17.963930479939172 val_loss: 14.584459099625137\n",
      "epoch 1 train_loss: 13.438651535648951 val_loss: 12.349846828564603\n",
      "epoch 2 train_loss: 12.051769768019902 val_loss: 11.941606332857244\n",
      "epoch 3 train_loss: 11.857019957215646 val_loss: 11.826712996068528\n",
      "epoch 4 train_loss: 11.809718334721028 val_loss: 11.803559780264283\n",
      "epoch 5 train_loss: 11.79210096273483 val_loss: 11.807636990035828\n",
      "epoch 6 train_loss: 11.782044771883303 val_loss: 11.801319844188503\n",
      "epoch 7 train_loss: 11.774535663458053 val_loss: 11.795464758841721\n",
      "epoch 8 train_loss: 11.765106092558085 val_loss: 11.81280628045145\n",
      "epoch 9 train_loss: 11.753145439729169 val_loss: 11.841124374769892\n",
      "epoch 10 train_loss: 11.740133361166183 val_loss: 11.867505431521277\n",
      "epoch 11 train_loss: 11.728924004940817 val_loss: 11.882868023599299\n",
      "epoch 12 train_loss: 11.719075653613519 val_loss: 11.879265081320236\n",
      "epoch 13 train_loss: 11.708039721135371 val_loss: 11.869204390347953\n",
      "epoch 14 train_loss: 11.694039310308632 val_loss: 11.845842285437516\n",
      "epoch 15 train_loss: 11.676752743394847 val_loss: 11.835304198087895\n",
      "epoch 16 train_loss: 11.65643894723142 val_loss: 11.810036611773166\n",
      "epoch 17 train_loss: 11.63472988947835 val_loss: 11.77448889034374\n",
      "epoch 18 train_loss: 11.612389931874134 val_loss: 11.726991880097755\n",
      "epoch 19 train_loss: 11.590828122430619 val_loss: 11.666375742982165\n",
      "epoch 20 train_loss: 11.566154681257071 val_loss: 11.598230326044643\n",
      "epoch 21 train_loss: 11.539631200205694 val_loss: 11.53141133901028\n",
      "epoch 22 train_loss: 11.511906658704563 val_loss: 11.47575332844432\n",
      "epoch 23 train_loss: 11.484936714016126 val_loss: 11.441675289320102\n",
      "epoch 24 train_loss: 11.460061776008308 val_loss: 11.411706978139092\n",
      "epoch 25 train_loss: 11.431253164994887 val_loss: 11.386256970836163\n",
      "epoch 26 train_loss: 11.399061632948047 val_loss: 11.361596864531222\n",
      "epoch 27 train_loss: 11.361776383184901 val_loss: 11.32719837952872\n",
      "epoch 28 train_loss: 11.320402055173982 val_loss: 11.332514991834191\n",
      "epoch 29 train_loss: 11.272342724584577 val_loss: 11.2982437022967\n",
      "epoch 30 train_loss: 11.229178483530807 val_loss: 11.264612507716437\n",
      "epoch 31 train_loss: 11.186201953348421 val_loss: 11.230434896339041\n",
      "epoch 32 train_loss: 11.141716041719691 val_loss: 11.189131203206333\n",
      "epoch 33 train_loss: 11.092493431093482 val_loss: 11.118908331877334\n",
      "epoch 34 train_loss: 11.047735026243743 val_loss: 11.047043370256144\n",
      "epoch 35 train_loss: 10.996745910604787 val_loss: 10.967283074369668\n",
      "epoch 36 train_loss: 10.947639972030414 val_loss: 10.884331084578017\n",
      "epoch 37 train_loss: 10.895732975896594 val_loss: 10.823912552856184\n",
      "epoch 38 train_loss: 10.841601171117137 val_loss: 10.768381432353065\n",
      "epoch 39 train_loss: 10.7658741590794 val_loss: 10.84392637048675\n",
      "epoch 40 train_loss: 10.736724853413826 val_loss: 10.748048069528702\n",
      "epoch 41 train_loss: 10.701800593417884 val_loss: 10.832529236268474\n",
      "epoch 42 train_loss: 10.681977861037229 val_loss: 10.690084849508812\n",
      "epoch 43 train_loss: 10.631423040026082 val_loss: 10.648839205955815\n",
      "epoch 44 train_loss: 10.605120139684301 val_loss: 10.65021881956451\n",
      "epoch 45 train_loss: 10.583652922118553 val_loss: 10.670638442911253\n",
      "epoch 46 train_loss: 10.559687744102238 val_loss: 10.666662701035683\n",
      "epoch 47 train_loss: 10.543004836716612 val_loss: 10.788701655121312\n",
      "epoch 48 train_loss: 10.52322645469468 val_loss: 10.846669785725876\n",
      "epoch 49 train_loss: 10.502458569888873 val_loss: 10.854488492085867\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city: miami\n",
      "epoch 0 train_loss: 19.455409680148247 val_loss: 14.461678253966069\n",
      "epoch 1 train_loss: 13.18652133646385 val_loss: 12.16248636498212\n",
      "epoch 2 train_loss: 11.939922714756987 val_loss: 11.787688257315695\n",
      "epoch 3 train_loss: 11.74589227131155 val_loss: 11.670996200096729\n",
      "epoch 4 train_loss: 11.663350534125748 val_loss: 11.627033948343495\n",
      "epoch 5 train_loss: 11.615949509458055 val_loss: 11.57080647478613\n",
      "epoch 6 train_loss: 11.578636559852862 val_loss: 11.527258110307866\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_456/2316083738.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(pred, opt, train_dataset, train_loader, val_dataset, val_loader)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    108\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "#train city models\n",
    "for city in cities:\n",
    "    print('city: ' + city)\n",
    "    batch_sz = 128  # batch size\n",
    "    train_dataset  = ArgoverseDataset(city = city, split = 'train')\n",
    "    train_loader = DataLoader(train_dataset,batch_size=batch_sz)\n",
    "    val_dataset = ArgoverseDataset(city = city, split = 'val')\n",
    "    val_loader = DataLoader(val_dataset,batch_size=batch_sz)\n",
    "    \n",
    "    pred = Pred()\n",
    "    opt = optim.Adam(pred.parameters(), lr=2e-4)\n",
    "    train(pred, opt, train_dataset, train_loader, val_dataset, val_loader)\n",
    "    pickle.dump(pred, open('models/ta_model_baseline_' + city, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "458fbe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cols = np.array(['v' + str(i) for i in range(120)])\n",
    "all_preds = []\n",
    "for city in cities:\n",
    "    load_pred = pickle.load(open('models/ta_model_baseline_' + city, 'rb'))\n",
    "    test_dataset = get_city_trajectories(city = city, split = 'test')\n",
    "    device = torch.device('cuda:0')\n",
    "    load_pred = load_pred.to(device)\n",
    "    preds = load_pred(torch.from_numpy(test_dataset[0]).to(device))\n",
    "    preds_reshaped = preds.reshape(preds.size()[0], 120)\n",
    "    preds_numpy = preds_reshaped.cpu().detach().numpy()\n",
    "    ids = np.array([str(i) + '_' + city for i in range(len(preds_numpy))])\n",
    "    predictions = pd.DataFrame(preds_numpy, columns=cols)\n",
    "    predictions.insert(0, 'ID', ids)\n",
    "    all_preds.append(predictions)\n",
    "    \n",
    "all_predictions = pd.concat(all_preds, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0dcf133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v110</th>\n",
       "      <th>v111</th>\n",
       "      <th>v112</th>\n",
       "      <th>v113</th>\n",
       "      <th>v114</th>\n",
       "      <th>v115</th>\n",
       "      <th>v116</th>\n",
       "      <th>v117</th>\n",
       "      <th>v118</th>\n",
       "      <th>v119</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_austin</td>\n",
       "      <td>-31.452484</td>\n",
       "      <td>-562.548340</td>\n",
       "      <td>-33.952343</td>\n",
       "      <td>-565.756409</td>\n",
       "      <td>-32.007393</td>\n",
       "      <td>-564.075317</td>\n",
       "      <td>-30.762920</td>\n",
       "      <td>-563.210083</td>\n",
       "      <td>-31.028749</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.905733</td>\n",
       "      <td>-564.923950</td>\n",
       "      <td>-32.423668</td>\n",
       "      <td>-565.540039</td>\n",
       "      <td>-32.022629</td>\n",
       "      <td>-564.601624</td>\n",
       "      <td>-32.312786</td>\n",
       "      <td>-565.114441</td>\n",
       "      <td>-35.172150</td>\n",
       "      <td>-567.413391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_austin</td>\n",
       "      <td>-351.348969</td>\n",
       "      <td>-0.119122</td>\n",
       "      <td>-350.579590</td>\n",
       "      <td>0.138293</td>\n",
       "      <td>-349.517548</td>\n",
       "      <td>0.117330</td>\n",
       "      <td>-351.175507</td>\n",
       "      <td>-2.667399</td>\n",
       "      <td>-348.796692</td>\n",
       "      <td>...</td>\n",
       "      <td>-351.217377</td>\n",
       "      <td>-4.216088</td>\n",
       "      <td>-349.444855</td>\n",
       "      <td>1.408106</td>\n",
       "      <td>-347.734467</td>\n",
       "      <td>0.657192</td>\n",
       "      <td>-353.903961</td>\n",
       "      <td>-2.523358</td>\n",
       "      <td>-352.531555</td>\n",
       "      <td>-3.875669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_austin</td>\n",
       "      <td>52.536083</td>\n",
       "      <td>-247.152374</td>\n",
       "      <td>52.810318</td>\n",
       "      <td>-246.935181</td>\n",
       "      <td>52.799950</td>\n",
       "      <td>-247.529129</td>\n",
       "      <td>52.189980</td>\n",
       "      <td>-247.396378</td>\n",
       "      <td>52.836784</td>\n",
       "      <td>...</td>\n",
       "      <td>53.448818</td>\n",
       "      <td>-247.667999</td>\n",
       "      <td>53.442268</td>\n",
       "      <td>-247.301743</td>\n",
       "      <td>52.962234</td>\n",
       "      <td>-247.433365</td>\n",
       "      <td>53.182888</td>\n",
       "      <td>-247.676529</td>\n",
       "      <td>53.037708</td>\n",
       "      <td>-247.476486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3_austin</td>\n",
       "      <td>-106.558907</td>\n",
       "      <td>1803.800537</td>\n",
       "      <td>-105.225006</td>\n",
       "      <td>1802.591431</td>\n",
       "      <td>-104.679924</td>\n",
       "      <td>1800.107422</td>\n",
       "      <td>-106.711349</td>\n",
       "      <td>1798.814697</td>\n",
       "      <td>-106.220612</td>\n",
       "      <td>...</td>\n",
       "      <td>-98.779205</td>\n",
       "      <td>1801.653442</td>\n",
       "      <td>-103.487602</td>\n",
       "      <td>1800.609985</td>\n",
       "      <td>-102.373840</td>\n",
       "      <td>1803.712036</td>\n",
       "      <td>-102.916985</td>\n",
       "      <td>1799.536621</td>\n",
       "      <td>-102.170425</td>\n",
       "      <td>1802.438843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_austin</td>\n",
       "      <td>1230.415649</td>\n",
       "      <td>-646.340393</td>\n",
       "      <td>1228.973511</td>\n",
       "      <td>-644.798462</td>\n",
       "      <td>1229.457275</td>\n",
       "      <td>-643.078735</td>\n",
       "      <td>1231.996826</td>\n",
       "      <td>-644.133118</td>\n",
       "      <td>1231.857666</td>\n",
       "      <td>...</td>\n",
       "      <td>1231.125122</td>\n",
       "      <td>-641.399292</td>\n",
       "      <td>1232.445557</td>\n",
       "      <td>-644.789551</td>\n",
       "      <td>1230.611938</td>\n",
       "      <td>-642.621582</td>\n",
       "      <td>1231.720337</td>\n",
       "      <td>-645.117065</td>\n",
       "      <td>1231.885620</td>\n",
       "      <td>-640.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29838</th>\n",
       "      <td>1681_palo-alto</td>\n",
       "      <td>-1385.287231</td>\n",
       "      <td>-460.426483</td>\n",
       "      <td>-1380.776733</td>\n",
       "      <td>-461.190033</td>\n",
       "      <td>-1380.695679</td>\n",
       "      <td>-462.279510</td>\n",
       "      <td>-1384.988525</td>\n",
       "      <td>-461.701294</td>\n",
       "      <td>-1383.289429</td>\n",
       "      <td>...</td>\n",
       "      <td>-1368.622192</td>\n",
       "      <td>-469.239227</td>\n",
       "      <td>-1379.109741</td>\n",
       "      <td>-455.256287</td>\n",
       "      <td>-1376.202026</td>\n",
       "      <td>-466.640106</td>\n",
       "      <td>-1363.880127</td>\n",
       "      <td>-459.745361</td>\n",
       "      <td>-1375.023804</td>\n",
       "      <td>-463.403198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29839</th>\n",
       "      <td>1682_palo-alto</td>\n",
       "      <td>128.678925</td>\n",
       "      <td>-37.629963</td>\n",
       "      <td>129.935883</td>\n",
       "      <td>-38.154217</td>\n",
       "      <td>129.317322</td>\n",
       "      <td>-37.539192</td>\n",
       "      <td>128.578171</td>\n",
       "      <td>-37.556068</td>\n",
       "      <td>128.766876</td>\n",
       "      <td>...</td>\n",
       "      <td>129.638794</td>\n",
       "      <td>-37.775154</td>\n",
       "      <td>130.258606</td>\n",
       "      <td>-35.779835</td>\n",
       "      <td>128.943008</td>\n",
       "      <td>-36.716232</td>\n",
       "      <td>129.099106</td>\n",
       "      <td>-36.260242</td>\n",
       "      <td>129.076157</td>\n",
       "      <td>-37.493565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29840</th>\n",
       "      <td>1683_palo-alto</td>\n",
       "      <td>-1450.032471</td>\n",
       "      <td>2170.785889</td>\n",
       "      <td>-1451.728516</td>\n",
       "      <td>2174.138672</td>\n",
       "      <td>-1447.593018</td>\n",
       "      <td>2174.030518</td>\n",
       "      <td>-1450.204956</td>\n",
       "      <td>2171.910400</td>\n",
       "      <td>-1451.995361</td>\n",
       "      <td>...</td>\n",
       "      <td>-1444.619385</td>\n",
       "      <td>2177.399170</td>\n",
       "      <td>-1445.512207</td>\n",
       "      <td>2177.559814</td>\n",
       "      <td>-1445.637939</td>\n",
       "      <td>2177.074951</td>\n",
       "      <td>-1447.102295</td>\n",
       "      <td>2174.739746</td>\n",
       "      <td>-1446.107178</td>\n",
       "      <td>2176.353760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29841</th>\n",
       "      <td>1684_palo-alto</td>\n",
       "      <td>1048.192139</td>\n",
       "      <td>1382.367310</td>\n",
       "      <td>1046.368164</td>\n",
       "      <td>1398.683350</td>\n",
       "      <td>1048.355347</td>\n",
       "      <td>1384.482910</td>\n",
       "      <td>1051.488770</td>\n",
       "      <td>1388.268311</td>\n",
       "      <td>1055.826416</td>\n",
       "      <td>...</td>\n",
       "      <td>1044.473389</td>\n",
       "      <td>1392.719727</td>\n",
       "      <td>1047.746948</td>\n",
       "      <td>1390.432129</td>\n",
       "      <td>1051.738647</td>\n",
       "      <td>1399.739868</td>\n",
       "      <td>1042.208130</td>\n",
       "      <td>1396.866577</td>\n",
       "      <td>1051.914917</td>\n",
       "      <td>1390.431641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29842</th>\n",
       "      <td>1685_palo-alto</td>\n",
       "      <td>-67.455780</td>\n",
       "      <td>443.354065</td>\n",
       "      <td>-67.213112</td>\n",
       "      <td>443.221497</td>\n",
       "      <td>-67.405052</td>\n",
       "      <td>441.765900</td>\n",
       "      <td>-67.236221</td>\n",
       "      <td>442.978241</td>\n",
       "      <td>-64.175888</td>\n",
       "      <td>...</td>\n",
       "      <td>-65.333839</td>\n",
       "      <td>445.994415</td>\n",
       "      <td>-64.983643</td>\n",
       "      <td>442.110321</td>\n",
       "      <td>-64.848579</td>\n",
       "      <td>444.341888</td>\n",
       "      <td>-65.824318</td>\n",
       "      <td>444.190491</td>\n",
       "      <td>-64.490982</td>\n",
       "      <td>444.711212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29843 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID           v0           v1           v2           v3  \\\n",
       "0            0_austin   -31.452484  -562.548340   -33.952343  -565.756409   \n",
       "1            1_austin  -351.348969    -0.119122  -350.579590     0.138293   \n",
       "2            2_austin    52.536083  -247.152374    52.810318  -246.935181   \n",
       "3            3_austin  -106.558907  1803.800537  -105.225006  1802.591431   \n",
       "4            4_austin  1230.415649  -646.340393  1228.973511  -644.798462   \n",
       "...               ...          ...          ...          ...          ...   \n",
       "29838  1681_palo-alto -1385.287231  -460.426483 -1380.776733  -461.190033   \n",
       "29839  1682_palo-alto   128.678925   -37.629963   129.935883   -38.154217   \n",
       "29840  1683_palo-alto -1450.032471  2170.785889 -1451.728516  2174.138672   \n",
       "29841  1684_palo-alto  1048.192139  1382.367310  1046.368164  1398.683350   \n",
       "29842  1685_palo-alto   -67.455780   443.354065   -67.213112   443.221497   \n",
       "\n",
       "                v4           v5           v6           v7           v8  ...  \\\n",
       "0       -32.007393  -564.075317   -30.762920  -563.210083   -31.028749  ...   \n",
       "1      -349.517548     0.117330  -351.175507    -2.667399  -348.796692  ...   \n",
       "2        52.799950  -247.529129    52.189980  -247.396378    52.836784  ...   \n",
       "3      -104.679924  1800.107422  -106.711349  1798.814697  -106.220612  ...   \n",
       "4      1229.457275  -643.078735  1231.996826  -644.133118  1231.857666  ...   \n",
       "...            ...          ...          ...          ...          ...  ...   \n",
       "29838 -1380.695679  -462.279510 -1384.988525  -461.701294 -1383.289429  ...   \n",
       "29839   129.317322   -37.539192   128.578171   -37.556068   128.766876  ...   \n",
       "29840 -1447.593018  2174.030518 -1450.204956  2171.910400 -1451.995361  ...   \n",
       "29841  1048.355347  1384.482910  1051.488770  1388.268311  1055.826416  ...   \n",
       "29842   -67.405052   441.765900   -67.236221   442.978241   -64.175888  ...   \n",
       "\n",
       "              v110         v111         v112         v113         v114  \\\n",
       "0       -31.905733  -564.923950   -32.423668  -565.540039   -32.022629   \n",
       "1      -351.217377    -4.216088  -349.444855     1.408106  -347.734467   \n",
       "2        53.448818  -247.667999    53.442268  -247.301743    52.962234   \n",
       "3       -98.779205  1801.653442  -103.487602  1800.609985  -102.373840   \n",
       "4      1231.125122  -641.399292  1232.445557  -644.789551  1230.611938   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "29838 -1368.622192  -469.239227 -1379.109741  -455.256287 -1376.202026   \n",
       "29839   129.638794   -37.775154   130.258606   -35.779835   128.943008   \n",
       "29840 -1444.619385  2177.399170 -1445.512207  2177.559814 -1445.637939   \n",
       "29841  1044.473389  1392.719727  1047.746948  1390.432129  1051.738647   \n",
       "29842   -65.333839   445.994415   -64.983643   442.110321   -64.848579   \n",
       "\n",
       "              v115         v116         v117         v118         v119  \n",
       "0      -564.601624   -32.312786  -565.114441   -35.172150  -567.413391  \n",
       "1         0.657192  -353.903961    -2.523358  -352.531555    -3.875669  \n",
       "2      -247.433365    53.182888  -247.676529    53.037708  -247.476486  \n",
       "3      1803.712036  -102.916985  1799.536621  -102.170425  1802.438843  \n",
       "4      -642.621582  1231.720337  -645.117065  1231.885620  -640.346008  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "29838  -466.640106 -1363.880127  -459.745361 -1375.023804  -463.403198  \n",
       "29839   -36.716232   129.099106   -36.260242   129.076157   -37.493565  \n",
       "29840  2177.074951 -1447.102295  2174.739746 -1446.107178  2176.353760  \n",
       "29841  1399.739868  1042.208130  1396.866577  1051.914917  1390.431641  \n",
       "29842   444.341888   -65.824318   444.190491   -64.490982   444.711212  \n",
       "\n",
       "[29843 rows x 121 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0731417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions.to_csv('out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab0b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('out.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
